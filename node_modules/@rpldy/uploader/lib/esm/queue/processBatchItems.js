import { triggerUpdater, isSamePropInArrays, FILE_STATES, logger, getMerge } from "@rpldy/shared";
import { UPLOADER_EVENTS } from "../consts";
import processFinishedRequest from "./processFinishedRequest";
var mergeWithUndefined = getMerge({
  undefinedOverwrites: true
});

var triggerPreSendUpdate = function (queue, items, options) {
  return triggerUpdater(queue.trigger, UPLOADER_EVENTS.REQUEST_PRE_SEND, {
    items: items,
    options: options
  }) // $FlowFixMe - https://github.com/facebook/flow/issues/8215
  .then(function (updated) {
    if (updated) {
      logger.debugLog("uploader.queue: REQUEST_PRE_SEND event returned updated items/options", updated);

      if (updated.items) {
        //can't change items count at this point.
        if (updated.items.length !== items.length || !isSamePropInArrays(updated.items, items, ["id", "batchId", "recycled"])) {
          throw new Error("REQUEST_PRE_SEND event handlers must return same items with same ids");
        }

        items = updated.items;
      }

      if (updated.options) {
        options = mergeWithUndefined({}, options, updated.options);
      }
    }

    return {
      items: items,
      options: options,
      cancelled: updated === false
    };
  });
};

var prepareAllowedItems = function (queue, items) {
  var allowedIds = items.map(function (item) {
    return item.id;
  });
  queue.updateState(function (state) {
    state.activeIds = state.activeIds.concat(allowedIds);
  });
  return triggerPreSendUpdate(queue, items, queue.getState().batches[items[0].batchId].batchOptions).then(function (prepared) {
    if (!prepared.cancelled) {
      //update potentially changed data back into queue state
      queue.updateState(function (state) {
        prepared.items.forEach(function (i) {
          state.items[i.id] = i;
        });
        state.batches[items[0].batchId].batchOptions = prepared.options;
      }); //use objects from internal state(proxies) - not objects from userland

      var updatedState = queue.getState();
      prepared.items = prepared.items.map(function (item) {
        return updatedState.items[item.id];
      });
      prepared.options = updatedState.batches[items[0].batchId].batchOptions;
    }

    return prepared;
  });
};

var updateUploadingState = function (queue, items, sendResult) {
  queue.updateState(function (state) {
    items.forEach(function (bi) {
      var item = state.items[bi.id];
      item.state = FILE_STATES.UPLOADING;
      state.aborts[bi.id] = sendResult.abort;
    });
  });
};

var sendAllowedItems = function (queue, itemsSendData, next) {
  var items = itemsSendData.items,
      options = itemsSendData.options;
  var batch = queue.getState().batches[items[0].batchId].batch;
  var sendResult;

  try {
    sendResult = queue.sender.send(items, batch, options);
  } catch (ex) {
    logger.debugLog("uploader.queue: sender failed with unexpected error", ex); //provide error result so file(s) are marked as failed

    sendResult = {
      request: Promise.resolve({
        status: 0,
        state: FILE_STATES.ERROR,
        response: ex.message
      }),
      abort: function abort() {
        return false;
      },
      senderType: "exception-handler"
    };
  }

  updateUploadingState(queue, items, sendResult);
  return sendResult.request //wait for server request to return
  .then(function (requestInfo) {
    var finishedData = items.map(function (item) {
      return {
        id: item.id,
        info: requestInfo
      };
    });
    processFinishedRequest(queue, finishedData, next);
  });
};

var reportCancelledItems = function (queue, items, cancelledResults, next) {
  var cancelledItemsIds = cancelledResults.map(function (isCancelled, index) {
    return isCancelled ? items[index].id : null;
  }).filter(Boolean);

  if (cancelledItemsIds.length) {
    var finishedData = cancelledItemsIds.map(function (id) {
      return {
        id: id,
        info: {
          status: 0,
          state: FILE_STATES.CANCELLED,
          response: "cancel"
        }
      };
    });
    processFinishedRequest(queue, finishedData, next); //report about cancelled items
  }

  return !!cancelledItemsIds.length;
}; //make sure item is still pending. Something might have changed while waiting for ITEM_START handling. Maybe someone called abort...


var getAllowedItem = function (id, queue) {
  return queue.getState().items[id];
}; //send group of items to be uploaded


var processBatchItems = function (queue, ids, next) {
  var state = queue.getState(); //ids will have more than one when grouping is allowed

  var items = Object.values(state.items);
  items = items.filter(function (item) {
    return !!~ids.indexOf(item.id);
  }); //allow user code cancel items from start event handler(s)

  return Promise.all(items.map(function (i) {
    return queue.runCancellable(UPLOADER_EVENTS.ITEM_START, i);
  })).then(function (cancelledResults) {
    var allowedItems = cancelledResults.map(function (isCancelled, index) {
      return isCancelled ? null : getAllowedItem(items[index].id, queue);
    }).filter(Boolean);
    return {
      allowedItems: allowedItems,
      cancelledResults: cancelledResults
    };
  }).then(function (_ref) {
    var allowedItems = _ref.allowedItems,
        cancelledResults = _ref.cancelledResults;
    var afterPreparePromise = allowedItems.length ? prepareAllowedItems(queue, allowedItems).then() : Promise.resolve();
    return afterPreparePromise.then(function (itemsSendData) {
      var nextP;

      if (itemsSendData) {
        if (itemsSendData.cancelled) {
          cancelledResults = ids.map(function () {
            return true;
          });
        } else {
          //we dont need to wait for the response here
          sendAllowedItems(queue, itemsSendData, next);
        }
      } //if no cancelled we can go to process more items immediately (and not wait for upload responses)


      if (!reportCancelledItems(queue, items, cancelledResults, next)) {
        nextP = next(queue); //when concurrent is allowed, we can go ahead and process more
      }

      return nextP;
    });
  });
};

export default processBatchItems;